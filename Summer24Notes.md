Hyperfocused on how does this represent him as a researcher/faculty member.

Do you think that if we put disclaimers that these are always indicative of everything for a, b, c, etc reasons how would they feel? Wouldn't get us there. Some stuff that may: no faculty page (specific faculty member); no public faculty specific stuff. 

Objective data

Just sorting them and suggesting that this person at the top is the best and the bottom is the worst



Journal Rankings

How long have they been here and compare those with similar people

You could compare a weighted of a long standing person and a new person to see how they compare. 

Formula
-> be open about what it is and how it's calculated
-> feedback/suggestions on what to take into account / how to take it into account
	-> survey

two steps (* = priority)
1) get internal one right*
2) going public



Notes on expanding it out to other universities.

Not the other priority but
- when talking to people from other universities they're excited
- potential to have some professors present to colleagues / other institutions
- if model/framework could be extended to other Maryland schools that would be ideal

Challenge:
- what's present right now is 60%

What graphs would be interesting:
Goal of project as a whole is to identify what is the universities research strong suits. This is to see where we should put concentrations.
Some things we were expecting were stem cells and AI but the data didn't show that. So charts need to move beyond the base data and help visualize features.
- Bar chart: categories (top 1, top 25, top ...)
- Scatter plots: num of articles and influence and what are the top categories based on that

For dashboard: at dean level want to know how many articles their faculty has published this year and how it compares to last year and other years. Can look at that from a university level and department level or school level. Can look at past 5 years and see what have we published on average that could allow for a **spedometer visualization** to show if we're on track for matching or being better than what we usually are.


Value: data and visualizations and the automation of getting those prior two things




PRIMARY NEXT STEPS
Perdue & Henson lists for last 5 years of articles. compare with data collected to see what is missing, get %.


-----------------------------------------------
Articles that are the most broad have the most articles

-1 level: University
0 level: School
Top Level: departments
Second Level: ?how do you make sure they are equally broad?





For some themes there's 
for example there's a theme of regression analysis, this isn't a theme it's a methodology.
Idea: scan for non-themes and correct output



Samples:
Article reference, faculty name, what AI said about it, abstract.
A couple different categories/schools/departments. Maybe 5 for Henson/Perdue and 2 for the other 3 in terms of articles and the related data.


----------------------------------------------------------------------
Summer work:
1. Check articles between WoS & Digital Measures
2. Analyze categories and how to level set them in terms of broadness
3. Send out samples
4. Flow diagram 1 step down (more specific) from the diagram on page 3
5. Test WoS API
-----------------------------------------------------------------------

Mike said he would reach out to me regarding meeting later.

Check-in meetings every other week: Tuesdays 10am